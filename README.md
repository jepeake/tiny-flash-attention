# _tiny flash attention_

_**[flash attention](https://github.com/Dao-AILab/flash-attention)** is a fast & memory-efficient exact attention algorithm that fuses operations into a single GPU kernel._

_**tiny flash attention** is a minimal implementation which expresses this in just ~20 lines of CUDA code._

_note: this is only the forward-pass._
